# Review_LLM_Robotics
基础模型在机器人认知与控制的应用研究综述

## 1.推理

- **Instruct2Act**:"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model",arXiV, May 2023,[[Paper](https://arxiv.org/abs/2305.11176)]  [[Code](https://github.com/OpenGVLab/Instruct2Act)]
- **TidyBot**: "TidyBot: Personalized Robot Assistance with Large Language Models", arXiV, May 2023, [[Paper](https://arxiv.org/abs/2305.05658)] [[Website](https://tidybot.cs.princeton.edu)]

- **PaLM-E**: "PaLM-E: An Embodied Multimodal Language Model", arXiV, Mar 2023. [[Paper](https://arxiv.org/abs/2303.03378)] [[Website](https://palm-e.github.io/)]

- **Grounded Decoding**:"Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control",arXiv,Mar 2023.[[Paper](https://arxiv.org/abs/2303.00855)] [[Website](https://grounded-decoding.github.io/)]

- **RT-1**: "RT-1: Robotics Transformer for Real-World Control at Scale", arXiv, Dec 2022. [[Paper](https://arxiv.org/abs/2212.06817)] [[Website](https://robotics-transformer1.github.io/)]

- **Housekeep**: "Housekeep: Tidying Virtual Households using Commonsense Reasoning", arXiv, May 2022. [[Paper](https://arxiv.org/abs/2205.10712)] [[Code](https://github.com/yashkant/housekeep)] [[Website](https://yashkant.github.io/housekeep/)]

- **Code-As-Policies**: "Code as Policies: Language Model Programs for Embodied Control", arXiv, Sept 2022.  [[Paper](https://arxiv.org/abs/2209.07753)] [[Code](https://github.com/google-research/google-research/tree/master/code_as_policies)] [[Website](https://code-as-policies.github.io)]

- **Say-Can**: "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", arXiv, Apr 2021. [[Paper](https://arxiv.org/abs/2204.01691)] [[Code](https://say-can.github.io/#open-source)] [[Website](https://say-can.github.io)]

- **Socratic**: "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", arXiv, Apr 2021. [[Paper](https://arxiv.org/abs/2204.00598)] [[Code](https://socraticmodels.github.io/#code)] [[Website](https://socraticmodels.github.io)]

  

## 2.规划

- "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction", arXiv, Mar 2023. [[Paper](https://arxiv.org/abs/2303.03548v1)] 

- **VLaMP**: "Pretrained Language Models as Visual Planners for Human Assistance", *arXiV, Apr 2023*, [[Paper](https://arxiv.org/abs/2304.09179)]

- **COWP**: "Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds", arXiv, May 2023. [[Paper](https://arxiv.org/abs/2305.17590)] [[Website](https://cowplanning.github.io/)] 

- **ChatGPT Robot Collaboration**: "Improved Trust in Human-Robot Collaboration with ChatGPT", arXiv, April 2023. [[Paper](https://arxiv.org/abs/2304.12529)]

- **ProgPrompt**: "Generating Situated Robot Task Plans using Large Language Models", arXiv, Sept 2022. [[Paper](https://arxiv.org/abs/2209.11302)] [[Website](https://progprompt.github.io/)]

- **LM-Nav**: "Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action", arXiv, July 2022. [[Paper](https://arxiv.org/abs/2207.04429)] [[Code](https://github.com/blazejosinski/lm_nav)] [[Website](https://sites.google.com/view/lmnav)]
 - **InnerMonlogue**: "Inner Monologue: Embodied Reasoning through Planning with Language Models", arXiv, July 2022. [[Paper](https://arxiv.org/abs/2207.05608)] [[Website](https://innermonologue.github.io/)]

- **VirtualHome**: "VirtualHome: Simulating Household Activities via Programs", arXiv,    Jun 2018.  [[Paper](https://arxiv.org/abs/1806.07011)] [[Code](https://github.com/xavierpuigf/virtualhome)] [[Website](http://virtual-home.org/)]

  

## 3.操控

- **VoxPoser**: "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models", arXiv, July 2023 [[Paper](https://arxiv.org/pdf/2307.05973.pdf)] [[Website](https://voxposer.github.io/)]

- **DIAL**:"Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models", *arXiv, Nov 2022*, [[Paper](https://arxiv.org/abs/2211.11736)] [[Website](https://instructionaugmentation.github.io/)]
- **R3M**:"R3M: A Universal Visual Representation for Robot Manipulation", *arXiv, Nov 2022*, [[Paper](https://arxiv.org/abs/2203.12601)] [[Code](https://github.com/facebookresearch/r3m)] [[Website](https://tinyurl.com/robotr3m)]

- **VIMA**:"VIMA: General Robot Manipulation with Multimodal Prompts", *arXiv, Oct 2022*, [[Paper](https://arxiv.org/abs/2210.03094)] [[Code](https://github.com/vimalabs/VIMA)] [[Website](https://vimalabs.github.io/)]
- **Perceiver-Actor**:"A Multi-Task Transformer for Robotic Manipulation", *CoRL, Sep 2022*. [[Paper](https://peract.github.io/paper/peract_corl2022.pdf)] [[Code](https://github.com/peract/peract)] [[Website](https://peract.github.io/)]
- **MOO**: "Open-World Object Manipulation using Pre-Trained Vision-Language Models", *arXiv, Mar 2022*. [[Paper](https://arxiv.org/abs/2303.00905)] [[Website](https://robot-moo.github.io/)]

- **CLIPort**: "CLIPort: What and Where Pathways for Robotic Manipulation", CoRL, Sept 2021. [[Paper](https://arxiv.org/abs/2109.12098)] [[Website](https://cliport.github.io/)]



## 4.导航

- **VLMaps**: "Visual Language Maps for Robot Navigation", arXiv, Mar 2023. [[Paper](https://arxiv.org/abs/2210.05714)] [[Code](https://github.com/PrieureDeSion/drive-any-robot)] [[Website](https://sites.google.com/view/drive-any-robot)]

- **GNM**:  "GNM: A General Navigation Model to Drive Any Robot", arXiv, Oct 2022. [[Paper](https://arxiv.org/abs/2210.03370)] [[Code](https://github.com/vlmaps/vlmaps)] [[Website](https://vlmaps.github.io/)]

- **CLIP-Nav**: "CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation", arXiv, Nov 2022. [[Paper](https://arxiv.org/abs/2211.16649)] 

- **ADAPT**: "ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts", CVPR, May 2022. [[Paper](https://arxiv.org/abs/2205.15509)]

- **CoW**: "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration", arXiv, Mar 2022. [[Paper](https://arxiv.org/abs/2203.10421)] [[Website](https://cow.cs.columbia.edu/)]

- **Recurrent VLN-BERT**: "A Recurrent Vision-and-Language BERT for Navigation", CVPR, Jun 2021 [[Paper](https://arxiv.org/abs/2011.13922)] [[Code](https://github.com/YicongHong/Recurrent-VLN-BERT)]

- **VLN-BERT**: "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web", ECCV, Apr 2020 [[Paper](https://arxiv.org/abs/2004.14973)] [[Code](https://github.com/arjunmajum/vln-bert)]

  

# 感谢以下链接
- [Everything-LLMs-And-Robotics](https://github.com/jrin771/Everything-LLMs-And-Robotics)

- [Awesome-LLM-Robotics](https://github.com/GT-RIPL/Awesome-LLM-Robotics)
